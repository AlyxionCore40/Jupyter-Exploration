{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlyxionCore40/Jupyter-Exploration/blob/main/L02_Alexia_Y_CHavez_ITAI_2373.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97QBHyofeV98"
      },
      "source": [
        "# Lab 02: Basic NLP Preprocessing Techniques\n",
        "\n",
        "**Course:** ITAI 2373 - Natural Language Processing  \n",
        "**Module:** 02 - Text Preprocessing  \n",
        "**Duration:** 2-3 hours  \n",
        "**Student Name:** ________________  \n",
        "**Date:** ________________\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Learning Objectives\n",
        "\n",
        "By completing this lab, you will:\n",
        "1. Understand the critical role of preprocessing in NLP pipelines\n",
        "2. Master fundamental text preprocessing techniques\n",
        "3. Compare different libraries and their approaches\n",
        "4. Analyze the effects of preprocessing on text data\n",
        "5. Build a complete preprocessing pipeline\n",
        "6. Load and work with different types of text datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-L6J3seV99"
      },
      "source": [
        "## üìñ Introduction to NLP Preprocessing\n",
        "\n",
        "Natural Language Processing (NLP) preprocessing refers to the initial steps taken to clean and transform raw text data into a format that's more suitable for analysis by machine learning algorithms.\n",
        "\n",
        "### Why is preprocessing crucial?\n",
        "\n",
        "1. **Standardization:** Ensures consistent text format across your dataset\n",
        "2. **Noise Reduction:** Removes irrelevant information that could confuse algorithms\n",
        "3. **Complexity Reduction:** Simplifies text to focus on meaningful patterns\n",
        "4. **Performance Enhancement:** Improves the efficiency and accuracy of downstream tasks\n",
        "\n",
        "### Real-world Impact\n",
        "Consider searching for \"running shoes\" vs \"Running Shoes!\" - without preprocessing, these might be treated as completely different queries. Preprocessing ensures they're recognized as equivalent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1AajM1neV99"
      },
      "source": [
        "### ü§î Conceptual Question 1\n",
        "**Before we start coding, think about your daily interactions with text processing systems (search engines, chatbots, translation apps). What challenges do you think these systems face when processing human language? List at least 3 specific challenges and explain why each is problematic.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Challenge 1: Ambiguity in human language ‚Äî Words and sentences often have multiple meanings depending on context (like ‚Äúbank‚Äù meaning the side of a river vs. a financial institution). This makes it hard for systems to always interpret meaning correctly.\n",
        "\n",
        "**Challenge 2:Figurative language and tone ‚Äî Idioms, sarcasm, and expressions like ‚Äúbreak a leg‚Äù don‚Äôt mean what they literally say, and NLP models struggle to understand these subtle meanings without deep context.\n",
        "\n",
        "**Challenge 3:Bias and data quality issues ‚Äî NLP models learn from large datasets that may contain biased, noisy, or unbalanced examples. This can lead to unfair or inaccurate outputs, especially in sensitive applications like hiring or legal decisions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXDC1YX1eV99"
      },
      "source": [
        "## üõ†Ô∏è Part 1: Environment Setup\n",
        "\n",
        "We'll be working with two major NLP libraries:\n",
        "- **NLTK (Natural Language Toolkit):** Comprehensive NLP library with extensive resources\n",
        "- **spaCy:** Industrial-strength NLP with pre-trained models\n",
        "\n",
        "**‚ö†Ô∏è Note:** Installation might take 2-3 minutes to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IjSAdym1eV99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569a2fcc-5238-4c7f-c002-287fe141aaa9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Installing NLP libraries...\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "‚úÖ Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Required Libraries\n",
        "print(\"üîß Installing NLP libraries...\")\n",
        "\n",
        "!pip install -q nltk spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0qYPPfkeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 2\n",
        "**Why do you think we need to install a separate language model (en_core_web_sm) for spaCy? What components might this model contain that help with text processing? Think about what information a computer needs to understand English text.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uqponRiceV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f07e94-41ec-4218-dbdc-51ad6e5935ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üì¶ Downloading NLTK data packages...\n",
            "\n",
            "‚úÖ All imports and downloads completed!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Import Libraries and Download NLTK Data\n",
        "import nltk\n",
        "import spacy\n",
        "import string\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Download essential NLTK data\n",
        "print(\"üì¶ Downloading NLTK data packages...\")\n",
        "nltk.download('punkt')      # For tokenization\n",
        "nltk.download('stopwords')  # For stop word removal\n",
        "nltk.download('wordnet')    # For lemmatization\n",
        "nltk.download('averaged_perceptron_tagger')  # For POS tagging\n",
        "\n",
        "print(\"\\n‚úÖ All imports and downloads completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQye-Ye2eV9-"
      },
      "source": [
        "## üìÇ Part 2: Sample Text Data\n",
        "\n",
        "We'll work with different types of text to understand how preprocessing affects various text styles:\n",
        "- Simple text\n",
        "- Academic text (with citations, URLs)\n",
        "- Social media text (with emojis, hashtags)\n",
        "- News text (formal writing)\n",
        "- Product reviews (informal, ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QNF6oBpFeV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f3d4a2-5465-4e53-d856-4b7fc8bc0282"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Sample texts loaded successfully!\n",
            "\n",
            "üè∑Ô∏è Simple: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "üè∑Ô∏è Academic: Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
            "She publi...\n",
            "\n",
            "üè∑Ô∏è Social Media: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yu...\n",
            "\n",
            "üè∑Ô∏è News: The stock market experienced significant volatility today, with tech stocks lead...\n",
            "\n",
            "üè∑Ô∏è Product Review: This laptop is absolutely fantastic! I've been using it for 6 months and it's st...\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Load Sample Texts\n",
        "simple_text = \"Natural Language Processing is a fascinating field of AI. It's amazing!\"\n",
        "\n",
        "academic_text = \"\"\"\n",
        "Dr. Smith's research on machine-learning algorithms is groundbreaking!\n",
        "She published 3 papers in 2023, focusing on deep neural networks (DNNs).\n",
        "The results were amazing - accuracy improved by 15.7%!\n",
        "\"This is revolutionary,\" said Prof. Johnson.\n",
        "Visit https://example.com for more info. #NLP #AI @university\n",
        "\"\"\"\n",
        "\n",
        "social_text = \"OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\"\n",
        "\n",
        "news_text = \"\"\"\n",
        "The stock market experienced significant volatility today, with tech stocks leading the decline.\n",
        "Apple Inc. (AAPL) dropped 3.2%, while Microsoft Corp. fell 2.8%.\n",
        "\"We're seeing a rotation out of growth stocks,\" said analyst Jane Doe from XYZ Capital.\n",
        "\"\"\"\n",
        "\n",
        "review_text = \"\"\"\n",
        "This laptop is absolutely fantastic! I've been using it for 6 months and it's still super fast.\n",
        "The battery life is incredible - lasts 8-10 hours easily.\n",
        "Only complaint: the keyboard could be better. Overall rating: 4.5/5 stars.\n",
        "\"\"\"\n",
        "\n",
        "# Store all texts\n",
        "sample_texts = {\n",
        "    \"Simple\": simple_text,\n",
        "    \"Academic\": academic_text.strip(),\n",
        "    \"Social Media\": social_text,\n",
        "    \"News\": news_text.strip(),\n",
        "    \"Product Review\": review_text.strip()\n",
        "}\n",
        "\n",
        "print(\"üìÑ Sample texts loaded successfully!\")\n",
        "for name, text in sample_texts.items():\n",
        "    preview = text[:80] + \"...\" if len(text) > 80 else text\n",
        "    print(f\"\\nüè∑Ô∏è {name}: {preview}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBOSIGXCeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 3\n",
        "**Looking at the different text types we've loaded, what preprocessing challenges do you anticipate for each type? For each text type below, identify at least 2 specific preprocessing challenges and explain why they might be problematic for NLP analysis.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "Simple text challenges:\n",
        "\n",
        "Few features or short content ‚Äî Simple text may be too short or minimal, making it hard for models to learn meaningful patterns.\n",
        "\n",
        "Limited context ‚Äî Short text can lack enough surrounding words for context-sensitive tasks like sentiment or entity recognition.\n",
        "\n",
        "Academic text challenges:\n",
        "\n",
        "Complex structure ‚Äî Academic text contains long sentences with technical jargon or formulas that are hard to tokenize and normalize.\n",
        "\n",
        "Domain terminology ‚Äî Specialized terms and abbreviations require custom handling or domain-specific token rules.\n",
        "\n",
        "Social media text challenges:\n",
        "\n",
        "Noisy language ‚Äî Slang, emojis, abbreviations, and typos make standard tokenization and normalization difficult.\n",
        "\n",
        "Informal grammar ‚Äî Lack of punctuation and irregular grammar can confuse models trained on formal text.\n",
        "\n",
        "News text challenges:\n",
        "\n",
        "Named entities and dates ‚Äî News often contains many dates, names, places, and numbers that need careful preprocessing (entity recognition vs normal tokens).\n",
        "\n",
        "Mixed formats ‚Äî Headlines and article bodies may include quotations, lists, or hyperlinks that complicate cleaning.\n",
        "\n",
        "Product review challenges:\n",
        "\n",
        "Subjective language ‚Äî Reviews often include sentiment-laden words or idioms that are hard to standardize.\n",
        "\n",
        "Spelling errors and repetition ‚Äî Repeated characters (‚Äúsoooo good‚Äù) and self-made spelling variations add noise.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLLnDF9YeV9-"
      },
      "source": [
        "## üî§ Part 3: Tokenization\n",
        "\n",
        "### What is Tokenization?\n",
        "Tokenization is the process of breaking down text into smaller, meaningful units called **tokens**. These tokens are typically words, but can also be sentences, characters, or subwords.\n",
        "\n",
        "### Why is it Important?\n",
        "- Most NLP algorithms work with individual tokens, not entire texts\n",
        "- It's the foundation for all subsequent preprocessing steps\n",
        "- Different tokenization strategies can significantly impact results\n",
        "\n",
        "### Common Challenges:\n",
        "- **Contractions:** \"don't\" ‚Üí \"do\" + \"n't\" or \"don't\"?\n",
        "- **Punctuation:** Keep with words or separate?\n",
        "- **Special characters:** How to handle @, #, URLs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "s3T8WpUheV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15095399-9cb9-40a2-94ec-83f8d4583c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç NLTK Tokenization Results\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Word tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "Number of tokens: 14\n",
            "\n",
            "Sentences: ['Natural Language Processing is a fascinating field of AI.', \"It's amazing!\"]\n",
            "Number of sentences: 2\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Tokenization with NLTK\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # needed in newer NLTK versions\n",
        "\n",
        "# Test on simple text\n",
        "print(\"üîç NLTK Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Word tokenization\n",
        "nltk_tokens = word_tokenize(simple_text)\n",
        "print(f\"\\nWord tokens: {nltk_tokens}\")\n",
        "print(f\"Number of tokens: {len(nltk_tokens)}\")\n",
        "\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(simple_text)\n",
        "print(f\"\\nSentences: {sentences}\")\n",
        "print(f\"Number of sentences: {len(sentences)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Wlre1ymeV9-"
      },
      "source": [
        "### ü§î Conceptual Question 4\n",
        "**Examine the NLTK tokenization results above. How did NLTK handle the contraction \"It's\"? What happened to the punctuation marks? Do you think this approach is appropriate for all NLP tasks? Explain your reasoning.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "How ‚ÄúIt‚Äôs‚Äù was handled:\n",
        "NLTK‚Äôs word_tokenize() splits contractions like ‚ÄúIt‚Äôs‚Äù into two tokens: ‚ÄúIt‚Äù and ‚Äú‚Äôs‚Äù. This is because the tokenizer follows Penn Treebank rules that break contractions into separate pieces.\n",
        "\n",
        "Punctuation treatment:\n",
        "Punctuation marks such as periods, commas, apostrophes, and question marks are treated as separate tokens, not part of the words. For example, ‚Äúhello!‚Äù becomes \"hello\" and \"!\" separately.\n",
        "\n",
        "Appropriateness for different tasks:\n",
        "This approach works well for linguistic analysis and tasks that need precise token boundaries (like parsing or POS tagging). But it may not be ideal for tasks like sentiment analysis where splitting contractions (e.g., ‚Äúdon‚Äôt‚Äù ‚Üí do n‚Äôt) can make recognizing negation harder. For simpler tasks, you might want to combine contractions or remove punctuation beforehand.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "TQKNF-yceV9-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe92339-5437-4829-9b89-07244ef2b08c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç spaCy Tokenization Results\n",
            "========================================\n",
            "Original: Natural Language Processing is a fascinating field of AI. It's amazing!\n",
            "\n",
            "Word tokens: ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "Number of tokens: 14\n",
            "\n",
            "üî¨ Detailed Token Analysis:\n",
            "Token        POS      Lemma        Is Alpha Is Stop \n",
            "--------------------------------------------------\n",
            "Natural      PROPN    Natural      1        0       \n",
            "Language     PROPN    Language     1        0       \n",
            "Processing   NOUN     processing   1        0       \n",
            "is           AUX      be           1        1       \n",
            "a            DET      a            1        1       \n",
            "fascinating  ADJ      fascinating  1        0       \n",
            "field        NOUN     field        1        0       \n",
            "of           ADP      of           1        1       \n",
            "AI           PROPN    AI           1        0       \n",
            ".            PUNCT    .            0        0       \n",
            "It           PRON     it           1        1       \n",
            "'s           AUX      be           0        1       \n",
            "amazing      ADJ      amazing      1        0       \n",
            "!            PUNCT    !            0        0       \n"
          ]
        }
      ],
      "source": [
        "# Step 5: Tokenization with spaCy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "print(\"üîç spaCy Tokenization Results\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "\n",
        "# Process with spaCy\n",
        "doc = nlp(simple_text)\n",
        "\n",
        "# Extract tokens\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(f\"\\nWord tokens: {spacy_tokens}\")\n",
        "print(f\"Number of tokens: {len(spacy_tokens)}\")\n",
        "\n",
        "# Show detailed token information\n",
        "print(f\"\\nüî¨ Detailed Token Analysis:\")\n",
        "print(f\"{'Token':<12} {'POS':<8} {'Lemma':<12} {'Is Alpha':<8} {'Is Stop':<8}\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<12} {token.pos_:<8} {token.lemma_:<12} {token.is_alpha:<8} {token.is_stop:<8}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCrQbVHceV9_"
      },
      "source": [
        "### ü§î Conceptual Question 5\n",
        "**Compare the NLTK and spaCy tokenization results. What differences do you notice? Which approach do you think would be better for different NLP tasks? Consider specific examples like sentiment analysis vs. information extraction.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "Key differences observed:\n",
        "NLTK and spaCy both split text into tokens, but spaCy‚Äôs tokenizer is part of a full NLP pipeline and uses a rule-based system that handles things like email addresses, URLs, and compound tokens more smoothly. NLTK‚Äôs tokenizers are more basic and may split on punctuation more often. spaCy also attaches linguistic annotations (like POS tags and lemmas) to tokens, while NLTK just returns token lists.\n",
        "\n",
        "Better for sentiment analysis:\n",
        "NLTK can work well for simple sentiment tasks, especially when combined with lexicons or basic classifiers. But spaCy‚Äôs richer annotations and consistent tokenization often help models understand context and word forms better, which can improve sentiment analysis quality.\n",
        "\n",
        "Better for information extraction:\n",
        "spaCy is generally better for information extraction because it includes built-in features like named entity recognition and syntactic structure that help identify people, places, dates, and relationships. NLTK would require extra tools and more manual steps to do the same.\n",
        "\n",
        "Overall assessment:\n",
        "NLTK is good for learning and simple preprocessing, while spaCy is more powerful and efficient for real-world applications that need deeper linguistic analysis and scalable performance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "kCVNmEgseV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68257d90-b478-4704-cc90-6e9dd938f26c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing on Social Media Text\n",
            "========================================\n",
            "Original: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\n",
            "\n",
            "NLTK tokens: ['OMG', '!', 'Just', 'tried', 'the', 'new', 'coffee', 'shop', '‚òïÔ∏è', 'SO', 'GOOD', '!', '!', '!', 'Highly', 'recommend', 'üëç', '#', 'coffee', '#', 'yum', 'üòç']\n",
            "spaCy tokens: ['OMG', '!', 'Just', 'tried', 'the', 'new', 'coffee', 'shop', '‚òï', 'Ô∏è', 'SO', 'GOOD', '!', '!', '!', 'Highly', 'recommend', 'üëç', '#', 'coffee', '#', 'yum', 'üòç']\n",
            "\n",
            "üìä Comparison:\n",
            "NLTK token count: 22\n",
            "spaCy token count: 23\n"
          ]
        }
      ],
      "source": [
        "# Step 6: Test Tokenization on Complex Text\n",
        "print(\"üß™ Testing on Social Media Text\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "# NLTK approach\n",
        "social_nltk_tokens = word_tokenize(social_text)\n",
        "print(f\"\\nNLTK tokens: {social_nltk_tokens}\")\n",
        "\n",
        "# spaCy approach\n",
        "social_doc = nlp(social_text)\n",
        "social_spacy_tokens = [token.text for token in social_doc]\n",
        "print(f\"spaCy tokens: {social_spacy_tokens}\")\n",
        "\n",
        "print(f\"\\nüìä Comparison:\")\n",
        "print(f\"NLTK token count: {len(social_nltk_tokens)}\")\n",
        "print(f\"spaCy token count: {len(social_spacy_tokens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOYfP6KqeV9_"
      },
      "source": [
        "### ü§î Conceptual Question 6\n",
        "**Looking at how the libraries handled social media text (emojis, hashtags), which library seems more robust for handling \"messy\" real-world text? What specific advantages do you notice? How might this impact a real-world application like social media sentiment analysis?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "More robust library:\n",
        "spaCy ‚Äî it‚Äôs built as a production‚Äëready NLP toolkit with a rule‚Äëbased, consistent tokenizer and full pipeline for linguistic analysis, making it better overall for messy real‚Äëworld text. üß†\n",
        "\n",
        "Specific advantages:\n",
        "\n",
        "spaCy‚Äôs tokenizer and pipeline are optimized for real text and scalable tasks, and it integrates tokenization with POS tagging, NER, and parsing.\n",
        "\n",
        "While NLTK has specialized tools (e.g., TweetTokenizer) that preserve emojis, hashtags, and mentions more cleanly for just tokenization, spaCy handles general noisy input within a richer framework and can be customized.\n",
        "\n",
        "Impact on sentiment analysis:\n",
        "Because sentiment analysis on social media depends on correctly capturing emojis, slang, hashtags, and user mentions, a robust tokenizer and pipeline help produce more meaningful and consistent tokens. spaCy‚Äôs efficient processing and rich annotations typically improve sentiment models‚Äô performance and scalability.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkfnnYyweV9_"
      },
      "source": [
        "## üõë Part 4: Stop Words Removal\n",
        "\n",
        "### What are Stop Words?\n",
        "Stop words are common words that appear frequently in a language but typically don't carry much meaningful information about the content. Examples include \"the\", \"is\", \"at\", \"which\", \"on\", etc.\n",
        "\n",
        "### Why Remove Stop Words?\n",
        "1. **Reduce noise** in the data\n",
        "2. **Improve efficiency** by reducing vocabulary size\n",
        "3. **Focus on content words** that carry semantic meaning\n",
        "\n",
        "### When NOT to Remove Stop Words?\n",
        "- **Sentiment analysis:** \"not good\" vs \"good\" - the \"not\" is crucial!\n",
        "- **Question answering:** \"What is the capital?\" - \"what\" and \"is\" provide context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "uWwuapZ0eV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9612480-c438-4f13-ba48-c5d1409ff6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä NLTK has 198 English stop words\n",
            "First 20: ['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been']\n",
            "\n",
            "üìä spaCy has 326 English stop words\n",
            "First 20: [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also']\n",
            "\n",
            "üîç Comparison:\n",
            "Common stop words: 123\n",
            "Only in NLTK: 75 - Examples: ['ain', 'aren', \"aren't\", 'couldn', \"couldn't\"]\n",
            "Only in spaCy: 203 - Examples: [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\"]\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Explore Stop Words Lists\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Get NLTK English stop words\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "print(f\"üìä NLTK has {len(nltk_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(nltk_stopwords))[:20]}\")\n",
        "\n",
        "# Get spaCy stop words\n",
        "spacy_stopwords = nlp.Defaults.stop_words\n",
        "print(f\"\\nüìä spaCy has {len(spacy_stopwords)} English stop words\")\n",
        "print(f\"First 20: {sorted(list(spacy_stopwords))[:20]}\")\n",
        "\n",
        "# Compare the lists\n",
        "common_stopwords = nltk_stopwords.intersection(spacy_stopwords)\n",
        "nltk_only = nltk_stopwords - spacy_stopwords\n",
        "spacy_only = spacy_stopwords - nltk_stopwords\n",
        "\n",
        "print(f\"\\nüîç Comparison:\")\n",
        "print(f\"Common stop words: {len(common_stopwords)}\")\n",
        "print(f\"Only in NLTK: {len(nltk_only)} - Examples: {sorted(list(nltk_only))[:5]}\")\n",
        "print(f\"Only in spaCy: {len(spacy_only)} - Examples: {sorted(list(spacy_only))[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4rQc88leV9_"
      },
      "source": [
        "### ü§î Conceptual Question 7\n",
        "**Why do you think NLTK and spaCy have different stop word lists? Look at the examples of words that are only in one list - do you agree with these choices? Can you think of scenarios where these differences might significantly impact your NLP results?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "Reasons for differences:\n",
        "\n",
        "Different design goals: NLTK and spaCy were created by different teams with different philosophies. NLTK is more academic and often favors broader coverage, while spaCy aims for efficient, production-ready NLP pipelines.\n",
        "\n",
        "Corpus and language assumptions: Each library may have used different text corpora to identify ‚Äúcommon‚Äù words, which affects which words are considered stop words.\n",
        "\n",
        "Update frequency and context: spaCy tends to update its list with a focus on modern, practical text processing, while NLTK‚Äôs list is older and more general.\n",
        "\n",
        "Agreement with choices:\n",
        "\n",
        "Words like ‚Äúfrom,‚Äù ‚Äúthe,‚Äù or ‚Äúis‚Äù are universally considered stop words and make sense to remove.\n",
        "\n",
        "Some words in one list but not the other (e.g., NLTK includes ‚Äúcould‚Äù but spaCy doesn‚Äôt) can be debated. Whether to treat modal verbs or pronouns as stop words depends on your task: in sentiment analysis, ‚Äúcould‚Äù might carry meaning.\n",
        "\n",
        "Scenarios where differences matter:\n",
        "\n",
        "Information retrieval: Including or excluding certain words could change search relevance or document similarity scores.\n",
        "\n",
        "Sentiment analysis or topic modeling: If spaCy keeps some words that NLTK removes, it could capture more nuanced meaning or context.\n",
        "\n",
        "Named entity recognition or syntax parsing: Overly aggressive stop word removal (like NLTK‚Äôs broader list) might remove important grammatical markers, slightly affecting parsing accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GrGWsVMReV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c122a8-72e9-48ca-8331-68e6dd5fdd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ NLTK Stop Word Removal\n",
            "========================================\n",
            "Original text:\n",
            "This is a simple sentence showing how stop words work.\n",
            "\n",
            "Original tokens (11): ['This', 'is', 'a', 'simple', 'sentence', 'showing', 'how', 'stop', 'words', 'work', '.']\n",
            "Filtered tokens (7): ['simple', 'sentence', 'showing', 'stop', 'words', 'work', '.']\n",
            "Removed words: ['This', 'is', 'a', 'how']\n",
            "Vocabulary reduction: 36.4%\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Remove Stop Words with NLTK\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 1. Define your text\n",
        "simple_text = \"This is a simple sentence showing how stop words work.\"\n",
        "\n",
        "# 2. Generate the missing variable\n",
        "nltk_tokens = word_tokenize(simple_text)\n",
        "\n",
        "# 3. Ensure your stopwords are also defined\n",
        "from nltk.corpus import stopwords\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# Assumptions:\n",
        "# nltk_tokens = list of tokens from your text (e.g., word_tokenize(simple_text))\n",
        "# nltk_stopwords = set of NLTK English stop words (e.g., set(stopwords.words('english')))\n",
        "\n",
        "# Remove stop words from the token list\n",
        "filtered_tokens = [word for word in nltk_tokens if word.lower() not in nltk_stopwords]\n",
        "\n",
        "# Identify which words were removed\n",
        "removed_words = [word for word in nltk_tokens if word.lower() in nltk_stopwords]\n",
        "\n",
        "# Calculate reduction in token count\n",
        "reduction = (len(nltk_tokens) - len(filtered_tokens)) / len(nltk_tokens) * 100\n",
        "\n",
        "# Display results\n",
        "print(\"üß™ NLTK Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original text:\\n{simple_text}\\n\")\n",
        "print(f\"Original tokens ({len(nltk_tokens)}): {nltk_tokens}\")\n",
        "print(f\"Filtered tokens ({len(filtered_tokens)}): {filtered_tokens}\")\n",
        "print(f\"Removed words: {removed_words}\")\n",
        "print(f\"Vocabulary reduction: {reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "lVuS0D-GeV9_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3106da4d-ced0-4633-aa6a-72a715e80538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ spaCy Stop Word Removal\n",
            "========================================\n",
            "Original: This is a simple sentence showing how stop words work.\n",
            "\n",
            "Original tokens (14): ['Natural', 'Language', 'Processing', 'is', 'a', 'fascinating', 'field', 'of', 'AI', '.', 'It', \"'s\", 'amazing', '!']\n",
            "After removing stop words & punctuation (6): ['simple', 'sentence', 'showing', 'stop', 'words', 'work']\n",
            "\n",
            "Removed words: ['This', 'is', 'a', 'how', '.']\n",
            "Vocabulary reduction: 57.1%\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Remove Stop Words with spaCy\n",
        "doc = nlp(simple_text)\n",
        "spacy_filtered = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(\"üß™ spaCy Stop Word Removal\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Original: {simple_text}\")\n",
        "print(f\"\\nOriginal tokens ({len(spacy_tokens)}): {spacy_tokens}\")\n",
        "print(f\"After removing stop words & punctuation ({len(spacy_filtered)}): {spacy_filtered}\")\n",
        "\n",
        "# Show which words were removed\n",
        "spacy_removed = [token.text for token in doc if token.is_stop or token.is_punct]\n",
        "print(f\"\\nRemoved words: {spacy_removed}\")\n",
        "\n",
        "# Calculate reduction percentage\n",
        "spacy_reduction = (len(spacy_tokens) - len(spacy_filtered)) / len(spacy_tokens) * 100\n",
        "print(f\"Vocabulary reduction: {spacy_reduction:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoVxWinyeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 8\n",
        "**Compare the NLTK and spaCy stop word removal results. Which approach removed more words? Do you think removing punctuation (as spaCy did) is always a good idea? Give a specific example where keeping punctuation might be important for NLP analysis.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "Which removed more:\n",
        "\n",
        "Typically, NLTK removes fewer words because its stop word list focuses only on common words like ‚Äúthe,‚Äù ‚Äúis,‚Äù ‚Äúand,‚Äù etc., while spaCy often removes more, including punctuation marks and some additional functional words.\n",
        "\n",
        "The actual counts will depend on your text, but spaCy‚Äôs extra removal of punctuation usually leads to a slightly greater reduction in token count.\n",
        "\n",
        "Punctuation removal assessment:\n",
        "\n",
        "Removing punctuation can be helpful when you only care about the core content words, e.g., for topic modeling or word frequency analysis.\n",
        "\n",
        "However, it is not always a good idea, because punctuation can carry semantic or structural meaning. For example, sentence boundaries, exclamation marks, and question marks can be critical for understanding tone, sentiment, or sentence splitting.\n",
        "\n",
        "Example where punctuation matters:\n",
        "\n",
        "Sentiment analysis: ‚ÄúI love this movie!‚Äù vs. ‚ÄúI love this movie‚Ä¶‚Äù ‚Äì the exclamation mark conveys enthusiasm, while ellipses convey hesitation or doubt. Removing punctuation here could change the interpreted sentiment.\n",
        "\n",
        "Parsing or named entity recognition: Commas can separate clauses or lists. Removing them might merge entities incorrectly, e.g., ‚ÄúApple, Inc., California‚Äù vs ‚ÄúApple Inc California.‚Äù\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUnesyUoeV-A"
      },
      "source": [
        "## üå± Part 5: Lemmatization and Stemming\n",
        "\n",
        "### What is Lemmatization?\n",
        "Lemmatization reduces words to their base or dictionary form (called a **lemma**). It considers context and part of speech to ensure the result is a valid word.\n",
        "\n",
        "### What is Stemming?\n",
        "Stemming reduces words to their root form by removing suffixes. It's faster but less accurate than lemmatization.\n",
        "\n",
        "### Key Differences:\n",
        "| Aspect | Stemming | Lemmatization |\n",
        "|--------|----------|---------------|\n",
        "| Speed | Fast | Slower |\n",
        "| Accuracy | Lower | Higher |\n",
        "| Output | May be non-words | Always valid words |\n",
        "| Context | Ignores context | Considers context |\n",
        "\n",
        "### Examples:\n",
        "- **\"running\"** ‚Üí Stem: \"run\", Lemma: \"run\"\n",
        "- **\"better\"** ‚Üí Stem: \"better\", Lemma: \"good\"\n",
        "- **\"was\"** ‚Üí Stem: \"wa\", Lemma: \"be\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RhQlU7wGeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ea4934-0263-4c97-f3bc-e92bd99d566c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåø Stemming Demonstration\n",
            "==============================\n",
            "Original     Stemmed     \n",
            "-------------------------\n",
            "running      run         \n",
            "runs         run         \n",
            "ran          ran         \n",
            "better       better      \n",
            "good         good        \n",
            "best         best        \n",
            "flying       fli         \n",
            "flies        fli         \n",
            "was          wa          \n",
            "were         were        \n",
            "cats         cat         \n",
            "dogs         dog         \n",
            "\n",
            "üß™ Applied to sample text:\n",
            "Original: ['This', 'is', 'a', 'simple', 'sentence', 'showing', 'how', 'stop', 'words', 'work']\n",
            "Stemmed: ['thi', 'is', 'a', 'simpl', 'sentenc', 'show', 'how', 'stop', 'word', 'work']\n"
          ]
        }
      ],
      "source": [
        "# Step 10: Stemming with NLTK\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Test words that demonstrate stemming challenges\n",
        "test_words = ['running', 'runs', 'ran', 'better', 'good', 'best', 'flying', 'flies', 'was', 'were', 'cats', 'dogs']\n",
        "\n",
        "print(\"üåø Stemming Demonstration\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12}\")\n",
        "print(\"-\" * 25)\n",
        "\n",
        "for word in test_words:\n",
        "    stemmed = stemmer.stem(word)\n",
        "    print(f\"{word:<12} {stemmed:<12}\")\n",
        "\n",
        "# Apply to our sample text\n",
        "sample_tokens = [token for token in nltk_tokens if token.isalpha()]\n",
        "stemmed_tokens = [stemmer.stem(token.lower()) for token in sample_tokens]\n",
        "\n",
        "print(f\"\\nüß™ Applied to sample text:\")\n",
        "print(f\"Original: {sample_tokens}\")\n",
        "print(f\"Stemmed: {stemmed_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1diItfT8eV-A"
      },
      "source": [
        "### ü§î Conceptual Question 9\n",
        "**Look at the stemming results above. Can you identify any cases where stemming produced questionable results? For example, how were \"better\" and \"good\" handled? Do you think this is problematic for NLP applications? Explain your reasoning.**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "**Questionable results identified:**\n",
        "\n",
        "**Assessment of \"better\" and \"good\":**\n",
        "\n",
        "**Impact on NLP applications:**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "VYhyO8jfeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07ea40ac-c832-4256-d8ff-57f753a78b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üå± spaCy Lemmatization Demonstration\n",
            "========================================\n",
            "Original: The researchers were studying the effects of running and swimming on better performance.\n",
            "\n",
            "Token           Lemma           POS        Explanation         \n",
            "-----------------------------------------------------------------\n",
            "The             the             DET        No change           \n",
            "researchers     researcher      NOUN       Lemmatized          \n",
            "were            be              AUX        Lemmatized          \n",
            "studying        study           VERB       Lemmatized          \n",
            "the             the             DET        No change           \n",
            "effects         effect          NOUN       Lemmatized          \n",
            "of              of              ADP        No change           \n",
            "running         run             VERB       Lemmatized          \n",
            "and             and             CCONJ      No change           \n",
            "swimming        swim            VERB       Lemmatized          \n",
            "on              on              ADP        No change           \n",
            "better          well            ADJ        Lemmatized          \n",
            "performance     performance     NOUN       No change           \n",
            "\n",
            "üî§ Lemmatized tokens (no stop words): ['researcher', 'study', 'effect', 'run', 'swim', 'well', 'performance']\n"
          ]
        }
      ],
      "source": [
        "# Step 11: Lemmatization with spaCy\n",
        "print(\"üå± spaCy Lemmatization Demonstration\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Test on a complex sentence\n",
        "complex_sentence = \"The researchers were studying the effects of running and swimming on better performance.\"\n",
        "doc = nlp(complex_sentence)\n",
        "\n",
        "print(f\"Original: {complex_sentence}\")\n",
        "print(f\"\\n{'Token':<15} {'Lemma':<15} {'POS':<10} {'Explanation':<20}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for token in doc:\n",
        "    if token.is_alpha:\n",
        "        explanation = \"No change\" if token.text.lower() == token.lemma_ else \"Lemmatized\"\n",
        "        print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {explanation:<20}\")\n",
        "\n",
        "# Extract lemmas\n",
        "lemmas = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]\n",
        "print(f\"\\nüî§ Lemmatized tokens (no stop words): {lemmas}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "tsCdhYHWeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32e927d6-e2c6-43e1-a1f4-fec595292046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚öñÔ∏è Stemming vs Lemmatization Comparison\n",
            "==================================================\n",
            "Original     Stemmed      Lemmatized  \n",
            "----------------------------------------\n",
            "better       better       well        \n",
            "running      run          run         \n",
            "studies      studi        study       \n",
            "was          wa           be          \n",
            "children     children     child       \n",
            "feet         feet         foot        \n"
          ]
        }
      ],
      "source": [
        "# Step 12: Compare Stemming vs Lemmatization\n",
        "comparison_words = ['better', 'running', 'studies', 'was', 'children', 'feet']\n",
        "\n",
        "print(\"‚öñÔ∏è Stemming vs Lemmatization Comparison\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"{'Original':<12} {'Stemmed':<12} {'Lemmatized':<12}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for word in comparison_words:\n",
        "    # Stemming\n",
        "    stemmed = stemmer.stem(word)\n",
        "\n",
        "    # Lemmatization with spaCy\n",
        "    doc = nlp(word)\n",
        "    lemmatized = doc[0].lemma_\n",
        "\n",
        "    print(f\"{word:<12} {stemmed:<12} {lemmatized:<12}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvNf3d6PeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 10\n",
        "**Compare the stemming and lemmatization results. Which approach do you think is more suitable for:**\n",
        "1. Search engine:\n",
        "\n",
        "Recommendation: Stemming\n",
        "\n",
        "Reasoning: Search engines prioritize speed and broad matching. Stemming reduces words to their root form quickly (e.g., ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù), which helps match different word forms without slowing down queries. Accuracy in preserving exact meaning is less critical because users often expect approximate matches.\n",
        "\n",
        "2. Sentiment analysis:\n",
        "\n",
        "Recommendation: Lemmatization\n",
        "\n",
        "Reasoning: Sentiment analysis relies on understanding the correct meaning of words. Lemmatization produces proper dictionary forms (e.g., ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù), which preserves semantics. Stemming could produce nonsensical roots (‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù, but ‚Äúbetter‚Äù ‚Üí ‚Äúbetter‚Äù), which may distort sentiment interpretation.\n",
        "\n",
        "3. Real-time chatbot:\n",
        "\n",
        "Recommendation: Combination, leaning toward lemmatization with lightweight processing\n",
        "\n",
        "Reasoning: Chatbots need both speed and accuracy. Lemmatization ensures responses are semantically correct, improving natural language understanding. However, lightweight or rule-based lemmatization is preferred to keep latency low. In some cases, stemming could be used for quick keyword matching, but full lemmatization generally yields better conversation quality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tep2Tdm4eV-A"
      },
      "source": [
        "## üßπ Part 6: Text Cleaning and Normalization\n",
        "\n",
        "### What is Text Cleaning?\n",
        "Text cleaning involves removing or standardizing elements that might interfere with analysis:\n",
        "- **Case normalization** (converting to lowercase)\n",
        "- **Punctuation removal**\n",
        "- **Number handling** (remove, replace, or normalize)\n",
        "- **Special character handling** (URLs, emails, mentions)\n",
        "- **Whitespace normalization**\n",
        "\n",
        "### Why is it Important?\n",
        "- Ensures consistency across your dataset\n",
        "- Reduces vocabulary size\n",
        "- Improves model performance\n",
        "- Handles edge cases in real-world data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "lmo2dAXkeV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d86afc0-a91c-4ea0-a943-91f8a738e379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üßπ Basic Text Cleaning\n",
            "==============================\n",
            "Original: '   Hello WORLD!!! This has 123 numbers and   extra spaces.   '\n",
            "Cleaned: 'hello world this has numbers and extra spaces'\n",
            "Length reduction: 26.2%\n"
          ]
        }
      ],
      "source": [
        "# Step 13: Basic Text Cleaning\n",
        "def basic_clean_text(text):\n",
        "    \"\"\"Apply basic text cleaning operations\"\"\"\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove extra spaces again\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test basic cleaning\n",
        "test_text = \"   Hello WORLD!!! This has 123 numbers and   extra spaces.   \"\n",
        "cleaned = basic_clean_text(test_text)\n",
        "\n",
        "print(\"üßπ Basic Text Cleaning\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Original: '{test_text}'\")\n",
        "print(f\"Cleaned: '{cleaned}'\")\n",
        "print(f\"Length reduction: {(len(test_text) - len(cleaned))/len(test_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FxHKo_2ceV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a2fc440-2e2a-43d5-a269-f2f94bd42e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Advanced Cleaning on Social Media Text\n",
            "=============================================\n",
            "Original: OMG! Just tried the new coffee shop ‚òïÔ∏è SO GOOD!!! Highly recommend üëç #coffee #yum üòç\n",
            "Cleaned: omg! just tried the new coffee shop ‚òïÔ∏è so good!!! highly recommend coffee yum\n",
            "Length reduction: 7.2%\n"
          ]
        }
      ],
      "source": [
        "# Step 14: Advanced Cleaning for Social Media\n",
        "def advanced_clean_text(text):\n",
        "    \"\"\"Apply advanced cleaning for social media and web text\"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "    # Remove mentions (@username)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Convert hashtags (keep the word, remove #)\n",
        "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
        "\n",
        "    # Remove emojis (basic approach)\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Convert to lowercase and normalize whitespace\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Test on social media text\n",
        "print(\"üöÄ Advanced Cleaning on Social Media Text\")\n",
        "print(\"=\" * 45)\n",
        "print(f\"Original: {social_text}\")\n",
        "\n",
        "cleaned_social = advanced_clean_text(social_text)\n",
        "print(f\"Cleaned: {cleaned_social}\")\n",
        "print(f\"Length reduction: {(len(social_text) - len(cleaned_social))/len(social_text)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwtcrJMVeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 11\n",
        "**Look at the advanced cleaning results for the social media text. What information was lost during cleaning? Can you think of scenarios where removing emojis and hashtags might actually hurt your NLP application? What about scenarios where keeping them would be beneficial?**\n",
        "\n",
        "Information lost:\n",
        "\n",
        "Emojis ‚Äì convey emotion, tone, or sentiment that plain text may not capture (e.g., üò¢ vs üôÇ).\n",
        "\n",
        "Hashtags ‚Äì often indicate topics, trends, or keywords that provide context.\n",
        "\n",
        "Mentions, URLs, and special symbols ‚Äì may indicate social interactions or references.\n",
        "\n",
        "Scenarios where removal hurts:\n",
        "\n",
        "Sentiment analysis: Removing emojis can lose crucial emotional cues, e.g., ‚ÄúI‚Äôm so happy üòÑ‚Äù becomes ‚ÄúI‚Äôm so happy,‚Äù which still works, but ‚ÄúI‚Äôm fine üò¢‚Äù loses the negative sentiment.\n",
        "\n",
        "Trend or topic detection: Removing hashtags like #ClimateChange removes explicit topic indicators that could help classify or cluster posts.\n",
        "\n",
        "User behavior modeling or social network analysis: Mentions and hashtags may signal interactions or communities; removing them erases this metadata.\n",
        "\n",
        "Scenarios where keeping helps:\n",
        "\n",
        "Emotion recognition / sentiment detection: Emojis are strong signals for positive, negative, or sarcastic emotions.\n",
        "\n",
        "Hashtag-based content categorization: Hashtags can serve as labels for supervised learning or trend tracking.\n",
        "\n",
        "Recommender systems: Preserving hashtags or mentions can improve personalization or content suggestions by leveraging social cues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpzKWVy3eV-A"
      },
      "source": [
        "## üîß Part 7: Building a Complete Preprocessing Pipeline\n",
        "\n",
        "Now let's combine everything into a comprehensive preprocessing pipeline that you can customize based on your needs.\n",
        "\n",
        "### Pipeline Components:\n",
        "1. **Text cleaning** (basic or advanced)\n",
        "2. **Tokenization** (NLTK or spaCy)\n",
        "3. **Stop word removal** (optional)\n",
        "4. **Lemmatization/Stemming** (optional)\n",
        "5. **Additional filtering** (length, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XxqZCHneV-A"
      },
      "outputs": [],
      "source": [
        "# Step 15: Complete Preprocessing Pipeline\n",
        "def preprocess_text(text,\n",
        "                   clean_level='basic',     # 'basic' or 'advanced'\n",
        "                   remove_stopwords=True,\n",
        "                   use_lemmatization=True,\n",
        "                   use_stemming=False,\n",
        "                   min_length=2):\n",
        "    \"\"\"\n",
        "    Complete text preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Step 1: Clean text\n",
        "    if clean_level == 'basic':\n",
        "        cleaned_text = basic_clean_text(text)\n",
        "    else:\n",
        "        cleaned_text = advanced_clean_text(text)\n",
        "\n",
        "    # Step 2: Tokenize\n",
        "    if use_lemmatization:\n",
        "        # Use spaCy for lemmatization\n",
        "        doc = nlp(cleaned_text)\n",
        "        tokens = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
        "    else:\n",
        "        # Use NLTK for basic tokenization\n",
        "        tokens = word_tokenize(cleaned_text)\n",
        "        tokens = [token for token in tokens if token.isalpha()]\n",
        "\n",
        "    # Step 3: Remove stop words\n",
        "    if remove_stopwords:\n",
        "        if use_lemmatization:\n",
        "            tokens = [token for token in tokens if token not in spacy_stopwords]\n",
        "        else:\n",
        "            tokens = [token.lower() for token in tokens if token.lower() not in nltk_stopwords]\n",
        "\n",
        "    # Step 4: Apply stemming if requested\n",
        "    if use_stemming and not use_lemmatization:\n",
        "        tokens = [stemmer.stem(token.lower()) for token in tokens]\n",
        "\n",
        "    # Step 5: Filter by length\n",
        "    tokens = [token for token in tokens if len(token) >= min_length]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "print(\"üîß Preprocessing Pipeline Created!\")\n",
        "print(\"‚úÖ Ready to test different configurations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "yYMlNDVceV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ca83b0-95fd-4cb4-f280-77c2b9f53e71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ Testing on: This laptop is absolutely fantastic! I've been using it for 6 months and it's still super fast.\n",
            "The ...\n",
            "============================================================\n",
            "\n",
            "1. Minimal processing (34 tokens):\n",
            "   ['this', 'laptop', 'is', 'absolutely', 'fantastic', 'ive', 'been', 'using', 'it', 'for']...\n",
            "\n",
            "2. Standard processing (18 tokens):\n",
            "   ['laptop', 'absolutely', 'fantastic', 've', 'use', 'month', 'super', 'fast', 'battery', 'life']...\n",
            "\n",
            "3. Aggressive processing (21 tokens):\n",
            "   ['laptop', 'absolut', 'fantast', 'use', 'month', 'still', 'super', 'fast', 'batteri', 'life']...\n",
            "\n",
            "üìä Token Reduction Summary:\n",
            "   Original: 47 tokens\n",
            "   Minimal: 34 (27.7% reduction)\n",
            "   Standard: 18 (61.7% reduction)\n",
            "   Aggressive: 21 (55.3% reduction)\n"
          ]
        }
      ],
      "source": [
        "# Step 16: Test Different Pipeline Configurations\n",
        "test_text = sample_texts[\"Product Review\"]\n",
        "print(f\"üéØ Testing on: {test_text[:100]}...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Configuration 1: Minimal processing\n",
        "minimal = preprocess_text(test_text,\n",
        "                         clean_level='basic',\n",
        "                         remove_stopwords=False,\n",
        "                         use_lemmatization=False,\n",
        "                         use_stemming=False)\n",
        "print(f\"\\n1. Minimal processing ({len(minimal)} tokens):\")\n",
        "print(f\"   {minimal[:10]}...\")\n",
        "\n",
        "# Configuration 2: Standard processing\n",
        "standard = preprocess_text(test_text,\n",
        "                          clean_level='basic',\n",
        "                          remove_stopwords=True,\n",
        "                          use_lemmatization=True)\n",
        "print(f\"\\n2. Standard processing ({len(standard)} tokens):\")\n",
        "print(f\"   {standard[:10]}...\")\n",
        "\n",
        "# Configuration 3: Aggressive processing\n",
        "aggressive = preprocess_text(test_text,\n",
        "                            clean_level='advanced',\n",
        "                            remove_stopwords=True,\n",
        "                            use_lemmatization=False,\n",
        "                            use_stemming=True,\n",
        "                            min_length=3)\n",
        "print(f\"\\n3. Aggressive processing ({len(aggressive)} tokens):\")\n",
        "print(f\"   {aggressive[:10]}...\")\n",
        "\n",
        "# Show reduction percentages\n",
        "original_count = len(word_tokenize(test_text))\n",
        "print(f\"\\nüìä Token Reduction Summary:\")\n",
        "print(f\"   Original: {original_count} tokens\")\n",
        "print(f\"   Minimal: {len(minimal)} ({(original_count-len(minimal))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Standard: {len(standard)} ({(original_count-len(standard))/original_count*100:.1f}% reduction)\")\n",
        "print(f\"   Aggressive: {len(aggressive)} ({(original_count-len(aggressive))/original_count*100:.1f}% reduction)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBb3LjQZeV-A"
      },
      "source": [
        "### ü§î Conceptual Question 12\n",
        "**Compare the three pipeline configurations (Minimal, Standard, Aggressive). For each configuration, analyze:**\n",
        "1. **What information was preserved?**\n",
        "2. **What information was lost?**\n",
        "3. **What type of NLP task would this configuration be best suited for?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "Minimal Processing:\n",
        "\n",
        "Preserved: All original text, punctuation, capitalization, emojis, hashtags, and formatting.\n",
        "\n",
        "Lost: Very little; only basic tokenization may occur.\n",
        "\n",
        "Best for: Tasks that rely on rich context or surface form, such as named entity recognition (NER), syntax parsing, or social media analysis where emojis and hashtags carry meaning.\n",
        "\n",
        "Standard Processing:\n",
        "\n",
        "Preserved: Core words, some punctuation may remain, basic structure intact. Stop words may be removed, text lowercased.\n",
        "\n",
        "Lost: Minor details like capitalization nuances, some common stop words, possibly some punctuation.\n",
        "\n",
        "Best for: General-purpose NLP tasks like text classification, topic modeling, or sentiment analysis, where removing noise improves model performance without losing essential meaning.\n",
        "\n",
        "Aggressive Processing:\n",
        "\n",
        "Preserved: Only key content words; punctuation, emojis, hashtags, stop words, and sometimes numbers are removed. Text is usually lowercased.\n",
        "\n",
        "Lost: Tone, emphasis, sentiment cues, fine-grained context, social or structural markers.\n",
        "\n",
        "Best for: Tasks focused purely on word-level frequency analysis, bag-of-words modeling, or information retrieval where reducing noise and standardizing vocabulary is more important than preserving subtle meaning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rtgwLN7neV-A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60d09e5-ccbb-4e89-f63d-dbcba95d21ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî¨ Comprehensive Preprocessing Analysis\n",
            "==================================================\n",
            "\n",
            "üìÑ Simple:\n",
            "   Original: 14 tokens\n",
            "   Processed: 7 tokens (50.0% reduction)\n",
            "   Sample: ['natural', 'language', 'processing', 'fascinating', 'field', 'ai', 'amazing']\n",
            "\n",
            "üìÑ Academic:\n",
            "   Original: 61 tokens\n",
            "   Processed: 26 tokens (57.4% reduction)\n",
            "   Sample: ['dr', 'smith', 'research', 'machinelearning', 'algorithm', 'groundbreake', 'publish', 'paper']\n",
            "\n",
            "üìÑ Social Media:\n",
            "   Original: 22 tokens\n",
            "   Processed: 10 tokens (54.5% reduction)\n",
            "   Sample: ['omg', 'try', 'new', 'coffee', 'shop', 'good', 'highly', 'recommend']\n",
            "\n",
            "üìÑ News:\n",
            "   Original: 51 tokens\n",
            "   Processed: 25 tokens (51.0% reduction)\n",
            "   Sample: ['stock', 'market', 'experience', 'significant', 'volatility', 'today', 'tech', 'stock']\n",
            "\n",
            "üìÑ Product Review:\n",
            "   Original: 47 tokens\n",
            "   Processed: 18 tokens (61.7% reduction)\n",
            "   Sample: ['laptop', 'absolutely', 'fantastic', 've', 'use', 'month', 'super', 'fast']\n",
            "\n",
            "\n",
            "üìã Summary Table\n",
            "Text Type       Original   Processed  Reduction \n",
            "--------------------------------------------------\n",
            "Simple          14         7          50.0      %\n",
            "Academic        61         26         57.4      %\n",
            "Social Media    22         10         54.5      %\n",
            "News            51         25         51.0      %\n",
            "Product Review  47         18         61.7      %\n"
          ]
        }
      ],
      "source": [
        "# Step 17: Comprehensive Analysis Across Text Types\n",
        "print(\"üî¨ Comprehensive Preprocessing Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test standard preprocessing on all text types\n",
        "results = {}\n",
        "for name, text in sample_texts.items():\n",
        "    original_tokens = len(word_tokenize(text))\n",
        "    processed_tokens = preprocess_text(text,\n",
        "                                      clean_level='basic',\n",
        "                                      remove_stopwords=True,\n",
        "                                      use_lemmatization=True)\n",
        "\n",
        "    reduction = (original_tokens - len(processed_tokens)) / original_tokens * 100\n",
        "    results[name] = {\n",
        "        'original': original_tokens,\n",
        "        'processed': len(processed_tokens),\n",
        "        'reduction': reduction,\n",
        "        'sample': processed_tokens[:8]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìÑ {name}:\")\n",
        "    print(f\"   Original: {original_tokens} tokens\")\n",
        "    print(f\"   Processed: {len(processed_tokens)} tokens ({reduction:.1f}% reduction)\")\n",
        "    print(f\"   Sample: {processed_tokens[:8]}\")\n",
        "\n",
        "# Summary table\n",
        "print(f\"\\n\\nüìã Summary Table\")\n",
        "print(f\"{'Text Type':<15} {'Original':<10} {'Processed':<10} {'Reduction':<10}\")\n",
        "print(\"-\" * 50)\n",
        "for name, data in results.items():\n",
        "    print(f\"{name:<15} {data['original']:<10} {data['processed']:<10} {data['reduction']:<10.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY3hCn_7eV-A"
      },
      "source": [
        "### ü§î Final Conceptual Question 13\n",
        "**Looking at the comprehensive analysis results across all text types:**\n",
        "\n",
        "1. **Which text type was most affected by preprocessing?** Why do you think this happened?\n",
        "\n",
        "2. **Which text type was least affected?** What does this tell you about the nature of that text?\n",
        "\n",
        "3. **If you were building an NLP system to analyze customer reviews for a business, which preprocessing approach would you choose and why?**\n",
        "\n",
        "4. **What are the main trade-offs you need to consider when choosing preprocessing techniques for any NLP project?**\n",
        "\n",
        "*Double-click this cell to write your answer:*\n",
        "\n",
        "1. Most affected text type:\n",
        "\n",
        "Social media text (e.g., tweets, posts)\n",
        "\n",
        "Reason: These texts contain emojis, hashtags, mentions, URLs, slang, abbreviations, and inconsistent punctuation/capitalization. Preprocessing removes or normalizes many of these elements, significantly reducing the original variability and content.\n",
        "\n",
        "2. Least affected text type:\n",
        "\n",
        "Formal text (e.g., news articles, academic writing)\n",
        "\n",
        "Reason: Formal texts have well-structured sentences, standard spelling, and limited use of emojis or special symbols. Preprocessing mainly removes stop words and lowercases text, so most of the core information remains intact.\n",
        "\n",
        "3. For customer review analysis:\n",
        "\n",
        "Recommended approach: Standard processing\n",
        "\n",
        "Reason: Reviews benefit from preserving sentiment and context while removing noise. Standard processing removes stop words, lowercases text, and optionally handles punctuation, but keeps key words and sentiment cues intact. Aggressive cleaning might remove important emotional or opinion words, while minimal cleaning may leave too much noise.\n",
        "\n",
        "4. Main trade-offs to consider:\n",
        "\n",
        "Information preservation vs. noise reduction: Aggressive cleaning reduces noise but may remove useful signals (emojis, punctuation, capitalization).\n",
        "\n",
        "Speed vs. accuracy: Lightweight preprocessing is faster but may be less semantically precise.\n",
        "\n",
        "Task specificity: Some tasks (NER, syntax parsing) need richer text; others (bag-of-words modeling, search) can work with aggressively cleaned text.\n",
        "\n",
        "Domain sensitivity: Social media, reviews, and informal texts require careful handling of non-standard language; formal texts are less sensitive.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byb97qokeV-G"
      },
      "source": [
        "## üéØ Lab Summary and Reflection\n",
        "\n",
        "Congratulations! You've completed a comprehensive exploration of NLP preprocessing techniques.\n",
        "\n",
        "### üîë Key Concepts You've Mastered:\n",
        "\n",
        "1. **Text Preprocessing Fundamentals** - Understanding why preprocessing is crucial\n",
        "2. **Tokenization Techniques** - NLTK vs spaCy approaches and their trade-offs\n",
        "3. **Stop Word Management** - When to remove them and when to keep them\n",
        "4. **Morphological Processing** - Stemming vs lemmatization for different use cases\n",
        "5. **Text Cleaning Strategies** - Basic vs advanced cleaning for different text types\n",
        "6. **Pipeline Design** - Building modular, configurable preprocessing systems\n",
        "\n",
        "### üéì Real-World Applications:\n",
        "These techniques form the foundation for search engines, chatbots, sentiment analysis, document classification, machine translation, and information extraction systems.\n",
        "\n",
        "### üí° Key Insights to Remember:\n",
        "- **No Universal Solution**: Different NLP tasks require different preprocessing approaches\n",
        "- **Trade-offs Are Everywhere**: Balance information preservation with noise reduction\n",
        "- **Context Matters**: The same technique can help or hurt depending on your use case\n",
        "- **Experimentation Is Key**: Always test and measure impact on your specific task\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work completing Lab 02!** üéâ\n",
        "\n",
        "For your reflection journal, focus on the insights you gained about when and why to use different techniques, the challenges you encountered, and connections you made to real-world applications."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}